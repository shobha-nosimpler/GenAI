{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq7fy8EBMEt74DBOyTw5hJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GenAI using gpt-3.0-turbo16k"
      ],
      "metadata": {
        "id": "09KBReVrCvDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-requisites\n",
        "1. Create an openai account at https://platform.openai.com\n",
        "\n",
        "2. From Dashboard --> API key : Create API key to make API requests to selected chat gpt version\n",
        "\n",
        "3. From Your Profile --> Billing --> Payment method : make the payment say 10 usd"
      ],
      "metadata": {
        "id": "fSfM6u7zDarM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install openai\n",
        "\n",
        "Inorder to use openai APIs install openai package , import openai and set the API key"
      ],
      "metadata": {
        "id": "-t91tPJ3DNRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## message\n",
        "\n",
        "request message is a disctionary with key value pairs. For example the value for key **role** is **system** if want to assign a role to chatgpt and pass another key **content** specifying what role it need to take for the query"
      ],
      "metadata": {
        "id": "mTaYv5DtHGnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Cases\n",
        "\n",
        "1. create message with two role: system and role: user\n",
        "  - create request with two parameters, model and messsage\n",
        "  - print the entire response\n",
        "  - print the response message\n",
        "\n",
        "2. modify the message to get response in JSON format\n",
        "  - print the output\n",
        "  - extract the key values from JSON output\n",
        "\n",
        "\n",
        "3. create prompt to specify the system role and clear instructions stating the specific requirments\n",
        "  - pass the prompt as the message content for the role user\n",
        "  - create the request with below parameters\n",
        "  -- model : gpt-3.0-turbo-16k\n",
        "  -- message : role:user, content:prompt\n",
        "  -- max_tokens : 200 (not clear), 800 (clear)\n",
        "  -- temperature : randomness in reponse (0 to 1)\n",
        "  -- stop : token at which to stop generating response\n",
        "  -- n : number of outputs\n",
        "  -- frequency_penalty : penalizes if tokens appear frequently\n",
        "  -- presence_penalty : penalizes if words that have been used are used again\n",
        "\n",
        "\n",
        "4. base_prompt with input variables\n"
      ],
      "metadata": {
        "id": "Aatsk2ImIUeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\"
      ],
      "metadata": {
        "id": "5icIgnPwFQ4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use cases\n",
        "-  who won IPL 2023 2022 2021 with only model and message parameters\n",
        "\n",
        "- promt\n",
        "creating prompt, and passing as multiline message\n",
        "passing 8 parameters m m m t n s f p\n",
        "\n",
        "model\n",
        "messages\n",
        "max_token - use 800, with 200 it looks like gibberish\n",
        "temperature - randomness in response\n",
        "n - number of outputs\n",
        "stop - stop generating response at this token, it is like exit command\n",
        "frequency_penalty - penalty for repetition in response\n",
        "presence_penalty - penalty for presence of token\n",
        "\n",
        "- get response as JSON output\n",
        "and extract the answer key\n",
        "- json format output is useful when you want to pass a value of key as input to another\n",
        "\n",
        "- base prompt with input variables\n",
        "The topic is : {0}\n",
        "Advantage is that you can pass it as input to a function"
      ],
      "metadata": {
        "id": "xiQ46rcOk1qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kUnrDPnEmMF1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pyre8LN2ba_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### install openai"
      ],
      "metadata": {
        "id": "-veCASV6hpAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "xVQfTC7zeyu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f31ebd2-90f1-480a-c68f-40ad638a390a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.6-py3-none-any.whl (327 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/327.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/327.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import openai"
      ],
      "metadata": {
        "id": "xspnxzeniREO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "aOHKfAQOe-zN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pass API key as input"
      ],
      "metadata": {
        "id": "zx4d-t6NiYw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pass secret key for authentication\n",
        "api_key = input(\"Enter your API key: \")\n",
        "openai.api_key = api_key"
      ],
      "metadata": {
        "id": "tBr9cvvniCWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Who won IPL 2021\n",
        "- gpt-3.0-turbo-16k was trained on data upto 2021 so it fails to answer for years after that"
      ],
      "metadata": {
        "id": "GRlLIIs9iGSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creat a simple message\n",
        "# message is a list of dictionaries with key value pairs\n",
        "# role:user content:some text\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Hi! You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won IPL 2021\"}\n",
        "    ]"
      ],
      "metadata": {
        "id": "u7_aroGfiTAU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    messages=messages\n",
        ")\n"
      ],
      "metadata": {
        "id": "PIEgkIaAjDOk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# below chat response for year 2023\n",
        "#chat_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3T6mJMXjQLN",
        "outputId": "f6e45c9f-84fd-46ba-ff86-0edd9111e683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9dVqLM9TfqYDrvhUyHBVjmAggempI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry, but as an AI language model, I don't have access to real-time information or future events. Therefore, I cannot provide you with the winner of IPL 2023 or any other future outcomes. I suggest checking reliable sports news sources or following the tournament to find out the winner.\", role='assistant', function_call=None, tool_calls=None))], created=1719204225, model='gpt-3.5-turbo-16k-0613', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=61, prompt_tokens=25, total_tokens=86))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# below chat response for year 2022\n",
        "#chat_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CTQtpYJj8A0",
        "outputId": "6183ab46-3b09-4045-f1b5-f8cccebd3fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9dVr00n7BMOUbXIvqiW2hpwPRTXJV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry, but as an AI language model, I don't have access to real-time data or the ability to browse the internet. Therefore, I cannot provide you with the answer to who won the IPL 2022. I recommend checking reliable news sources or conducting an internet search for the latest information.\", role='assistant', function_call=None, tool_calls=None))], created=1719204266, model='gpt-3.5-turbo-16k-0613', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=62, prompt_tokens=25, total_tokens=87))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# below chat response for year 2021\n",
        "chat_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OstsOgS4kUye",
        "outputId": "7c517580-a973-42e1-c150-d359deb7c46e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9dVtr7FyrqTsfCEHyZLt8Bq04Oqsc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Mumbai Indians won the IPL 2021.', role='assistant', function_call=None, tool_calls=None))], created=1719204443, model='gpt-3.5-turbo-16k-0613', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=25, total_tokens=35))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for year 2023\n",
        "#chat_response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "4E-Y80KGjTEd",
        "outputId": "bcb819c7-5287-4024-8b55-008599cca1d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, but as an AI language model, I don't have access to real-time information or future events. Therefore, I cannot provide you with the winner of IPL 2023 or any other future outcomes. I suggest checking reliable sports news sources or following the tournament to find out the winner.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for year 2022\n",
        "chat_response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "LUO2qnj0kIxL",
        "outputId": "f935b0d9-97c0-4aca-fa6e-4d8604e6ae25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, but as an AI language model, I don't have access to real-time data or the ability to browse the internet. Therefore, I cannot provide you with the answer to who won the IPL 2022. I recommend checking reliable news sources or conducting an internet search for the latest information.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for year 2021\n",
        "chat_response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YQ-ZPJ7EkZ9i",
        "outputId": "1a5f2da3-06d4-424e-d827-a5136910adc6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'IPL 2021, also known as the Indian Premier League 2021, was won by the Chennai Super Kings.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JSON output with key answer"
      ],
      "metadata": {
        "id": "3McJqsHvFPHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON to the key 'answer'.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won IPL 2020. Inclue year in response\"}\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "o3LAulJK36gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = chat_response.choices[0].message.content\n",
        "\n",
        "# Print the generated output\n",
        "print(output)\n",
        "\n",
        "# Print the type of the output (usually a string)\n",
        "print(\"Output Type:\", type(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_LgpjTJ4kW-",
        "outputId": "fdc549da-3a57-430c-db06-ae226d68d727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"answer\": {\n",
            "    \"year\": 2020,\n",
            "    \"winner\": \"Mumbai Indians\"\n",
            "  }\n",
            "}\n",
            "Output Type: <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the answer key\n",
        "import json\n",
        "json_output = json.loads(output)\n",
        "result = json_output.get(\"answer\", \"None\")\n",
        "\n",
        "# result has the value correspondeing to answer key in json output, None if key is not present\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz0clnTc4_YY",
        "outputId": "c01819f5-a059-4bb3-ac64-84b7f152d65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'year': 2020, 'winner': 'Mumbai Indians'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Prompt to pass in message"
      ],
      "metadata": {
        "id": "S-bYgIdBmNpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''You are a helpful Neural Network teaching assistant.\n",
        "Explain the various optimization methods in Neural Networks.\n",
        "Provide an exhaustive summary of the methods describing what they do,\n",
        "sample code for each, guidelines on when to use which method.\n",
        "'''"
      ],
      "metadata": {
        "id": "08RrnoH7miRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass prompt as message\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]"
      ],
      "metadata": {
        "id": "igDdYpddn_m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### set max tokens to 200\n",
        "Output looks gibberish"
      ],
      "metadata": {
        "id": "enZ28D9QjVCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    messages=messages,\n",
        "    max_tokens=200,\n",
        "    temperature=0.5,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")"
      ],
      "metadata": {
        "id": "m34MG0Vqq6DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat response with max tokens 200 - not well formated\n",
        "chat_response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "9WEkriMirHgq",
        "outputId": "c42e406d-ff62-4352-a2bb-a397597b2f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are several optimization methods used in Neural Networks to train the models effectively. Here is an exhaustive summary of some commonly used optimization methods, along with sample code and guidelines on when to use each method:\\n\\n1. Gradient Descent:\\n   - Gradient descent is a basic optimization method that updates the model parameters in the direction of steepest descent of the loss function.\\n   - It calculates the gradients of the parameters with respect to the loss and updates the parameters using a learning rate.\\n   - Sample code using TensorFlow:\\n     ```python\\n     optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\\n     with tf.GradientTape() as tape:\\n         predictions = model(inputs)\\n         loss = loss_fn(labels, predictions)\\n     gradients = tape.gradient(loss, model.trainable_variables)\\n     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\\n     ```\\n   - Use gradient descent when the dataset is large and the model has a small number of parameters.\\n\\n2. Stochastic'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### set max tokens to 800\n",
        "output is well formatted"
      ],
      "metadata": {
        "id": "KDeqjUW6jeQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6l-F4U23rb65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat response with max tokens 800 - well structured\n",
        "chat_response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    messages=messages,\n",
        "    max_tokens=800,\n",
        "    temperature=0.5,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNp7nje4rUW8",
        "outputId": "3cd151ff-ec94-41a4-ba8c-59bf3a2b5f73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are several optimization methods commonly used in Neural Networks to improve their performance and convergence. Here is an exhaustive summary of these methods, along with sample code and guidelines on when to use each method:\n",
            "\n",
            "1. Gradient Descent (GD):\n",
            "Gradient Descent is the most fundamental optimization method used in Neural Networks. It updates the weights of the network in the opposite direction of the gradient of the loss function with respect to the weights. The learning rate determines the step size in updating the weights.\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "learning_rate = 0.01\n",
            "for each epoch:\n",
            "    compute gradients\n",
            "    update weights: weights = weights - learning_rate * gradients\n",
            "```\n",
            "\n",
            "Guidelines:\n",
            "- GD is a good starting point for most problems.\n",
            "- It works well with small to medium-sized datasets.\n",
            "\n",
            "2. Stochastic Gradient Descent (SGD):\n",
            "SGD is an extension of GD where instead of computing gradients on the entire dataset, it computes gradients on a randomly selected subset of the data (mini-batch). This approach reduces computation time and can lead to faster convergence.\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "learning_rate = 0.01\n",
            "for each epoch:\n",
            "    shuffle(data)\n",
            "    for each mini-batch:\n",
            "        compute gradients\n",
            "        update weights: weights = weights - learning_rate * gradients\n",
            "```\n",
            "\n",
            "Guidelines:\n",
            "- SGD is suitable for large datasets.\n",
            "- It can help escape from local minima due to the randomness of mini-batches.\n",
            "\n",
            "3. Mini-Batch Gradient Descent:\n",
            "Mini-Batch GD is a compromise between GD and SGD. It computes gradients on a small batch of data instead of the entire dataset. This approach provides a balance between convergence speed and computational efficiency.\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "learning_rate = 0.01\n",
            "batch_size = 32\n",
            "for each epoch:\n",
            "    shuffle(data)\n",
            "    for i in range(0, len(data), batch_size):\n",
            "        mini_batch = data[i:i+batch_size]\n",
            "        compute gradients\n",
            "        update weights: weights = weights - learning_rate * gradients\n",
            "```\n",
            "\n",
            "Guidelines:\n",
            "- Mini-Batch GD is commonly used in practice for most problems.\n",
            "- It provides a good trade-off between convergence speed and computational efficiency.\n",
            "\n",
            "4. Momentum:\n",
            "Momentum helps accelerate the convergence by adding a fraction of the previous weight update to the current update. It helps the optimizer to navigate flat regions and accelerates convergence in the relevant direction.\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "learning_rate = 0.01\n",
            "momentum = 0.9\n",
            "velocity = 0\n",
            "for each epoch:\n",
            "    compute gradients\n",
            "    velocity = momentum * velocity + learning_rate * gradients\n",
            "    update weights: weights = weights - velocity\n",
            "```\n",
            "\n",
            "Guidelines:\n",
            "- Momentum can be useful in escaping local minima and accelerating convergence.\n",
            "- It is commonly used in practice for various problems.\n",
            "\n",
            "5. Adam:\n",
            "Adam (Adaptive Moment Estimation) combines the benefits of both Momentum and RMSprop. It adapts the learning rate for each parameter based on the first and second moments of the gradients. It performs well on a wide range of problems and provides faster convergence.\n",
            "\n",
            "Sample code:\n",
            "```python\n",
            "learning_rate = 0.01\n",
            "beta1 = 0.9\n",
            "beta2 = 0.999\n",
            "epsilon = 1e-8\n",
            "m = 0\n",
            "v = 0\n",
            "for each epoch:\n",
            "    compute gradients\n",
            "    m = beta1 * m + (1 - beta1) * gradients\n",
            "    v = beta2 * v + (1 - beta2) * (gradients ** 2)\n",
            "    m_hat = m / (1 - beta1)\n",
            "    v_hat = v / (1 - beta2)\n",
            "    update weights: weights = weights - learning_rate * (m_hat / (sqrt(v_hat) + epsilon))\n",
            "```\n",
            "\n",
            "Guidelines:\n",
            "- Adam is a popular choice for optimization due to its adaptive learning rate and good performance on various problems.\n",
            "- It is recommended to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eYplguNU07F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1W3nvudv07QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ooniH--x-4db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pYczpDRb07aj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}